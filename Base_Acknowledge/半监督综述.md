# 半监督学习综述

翻译自：An Overview of Deep Semi-Supervised Learning

## 摘要

半监督学习（SSL）已成为深度学习中令人兴奋的新研究方向，以应对很少有标记的培训示例以及大量未标记样本的情况，从而使其可用于实际的 世界上的应用程序，其中未标记的数据易于获得且易于获取，而标记的实例通常很难收集，昂贵且耗时。  SSL能够构建更好的分类器，以弥补缺少标签训练数据的不足。 但是，为了避免问题结构与模型假设的较差匹配（这可能导致分类性能下降），SSL仅在某些假设下才有效，例如，假设决策边界应避开高密度区域 ，便于从未标记的实例中提取其他信息以规范化培训。 在本文中，我们将首先介绍SSL及其主要假设和方法，然后概述深度学习中占主导地位的半监督方法。

## 介绍

在机器学习文献中，主要有三类方法用于解决结合标记数据和未标记数据从而提高性能的问题：**半监督学习（SSL），直推学习（transductive learning）和主动学习（active learning）**。**半监督学习**是指试图利用未标记数据进行监督学习（半监督分类）的方法，如图2所示，或在无监督学习（半监督聚类）中包含先验信息如类别标签、成对约束或聚类成员等。**直推学习**指的是也尝试利用未标记的示例，但是假设未标记的样本也是最终的测试样本。也就是说，测试集是事先已知的，并且学习的目标是优化在给定测试集上的分类性能。**主动学习**，有时称为选择性采样。能够从未标记数据中选择最重要的样本，然后让相关专家为这些数据做标注，其目的是最小化数据利用率。最流行的算法是不确定性抽样（US，uncertainty sampling）和委员会查询抽样（QBC，query by committee）。不确定性抽样训练一个分类器，然后查询分类器最不确定的未标记样本。委员会查询抽样构建多个分类器然后查询这些分类器歧义最大的未标记样本。

**主动学习：**学习算法主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注，这个筛选的过程就是主动学习主要研究的地方了。

**直推学习：**假设未标记的数据就是最终用来测试的数据，学习的目的就是在这些数据上取得最佳的泛化能力。相对应的，半监督学习在学习时并不知道最终的测试用例是什么。

**半监督学习：**直推学习其实类似于半监督学习的一个子问题。

<img src="https://img-blog.csdnimg.cn/20190906144825288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI2ODIyMDI5,size_16,color_FFFFFF,t_70" style="zoom: 67%;" />





## 半监督学习常用方法(只考虑CNN)

**Adversarial learning for semi-supervised semantic segmentation**

1.  利用生成对抗网络

这篇论文中，将generator换成一个**分割网络**(可以是任意形式的分割网络，如：FCN,DeepLab，DilatedNet……,输入是H*W*3,依次是长宽，通道数，输出概率图为H*W*C,其中C是语义种类数),这个网络对输入的图片分割输出一个概率图，使得输出的概率图尽可能的接近ground truth。**其中discriminator采用了全卷积网络（输入为generator或ground truth得到的概率图，输出位空间概率图H*W*1,其中其中像素点p代表这个来自gournd truth(p=1)还是generator(p=0)。**在训练中，用半监督机制，一部分是注解数据，一部分是无注解数据。当用有注解数据时，分割网络由基**于ground truth的标准交叉熵损失和基于鉴别器的对抗损失**共同监督。注意，**训练discriminator只用标记数据。当用无注解数据时，用半监督方法训练分割网络，在从分割网络中获取未标记图像的初始分割预测后，通过判别网络对分割预测进行传递，得到一个置信图。**我们反过来将这个置信图作为监督信号，使用一个自学机制来训练带masked交叉熵损失的分割网络。置信图表示了预测分割的质量。

启发: 同样的,也可以使用生成对抗网络,(**考虑利用本身的切面信息,有标签的样本,输入图片和利用本身的标注,最后预测分为分割图和切面分类; 而对于没有标注的样本,输入图像和**)

2. 

