# 梯度下降算法

[动手学深度学习](https://zh.d2l.ai/chapter_optimization/gd-sgd.html)

## 批量梯度下降

批量梯度下降是最原始的梯度下降方法，在没有mini_batch之前，在进行一次梯度更新时需要把所有的样本损失进行计算，最后求出它们的梯度，最后求平均。

$$
\nabla f(\boldsymbol{x})=\frac{1}{n} \sum_{i=1}^{n} \nabla f_{i}(\boldsymbol{x})
$$

t梯度的更新为：$\boldsymbol{x} \leftarrow \boldsymbol{x}-\eta \nabla f_{i}(\boldsymbol{x})$

## 随机梯度下降

由于批量梯度下降更新一次梯度就需要对所有样本进行损失计算并且对所有参数求导，收敛速度太过于缓慢，计算成本也很高。因此就提出了随机梯度下降。随机梯度下降就是每一次梯度更新都随机（分有放回/无放回）抽取一个样本对梯度进行更新。
$$
\boldsymbol{x} \leftarrow \boldsymbol{x}-\eta \nabla f_{i}(\boldsymbol{x})
$$
随机梯度下降的过程可能会相当曲折。

![](https://zh.d2l.ai/_images/chapter_optimization_gd-sgd_15_1.svg)

## mini_batch随机梯度下降

随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度，这实际上是我们最常用的一个梯度下降方法。

设置一个mini_batch_size=8,那么实际上在每一次的梯度更新过程对整个样本集进行了一个（通常是无放回）随机采样，采出8个样本，然后对这8个样本进行批量梯度下降。
$$
\boldsymbol{g}_{t} \leftarrow \nabla f_{\mathcal{B}_{t}}\left(\boldsymbol{x}_{t-1}\right)=\frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_{t}} \nabla f_{i}\left(\boldsymbol{x}_{t-1}\right)
$$
梯度更新：$\boldsymbol{x}_{t} \leftarrow \boldsymbol{x}_{t-1}-\eta_{t} \boldsymbol{g}_{t}$

## 动量法

## ADam法

