# 实例分割综述

主要分为自上而下的实例分割方法；和自下而上的实例分割方法。

**自上而下的实例分割方法**

思路是：首先通过目标检测的方法**找出实例所在的区域**（bounding box），再**在检测框内进行语义分割**，每个分割结果都作为一个不同的实例输出。

自上而下的密集实例分割的开山鼻祖是DeepMask，它通过滑动窗口的方法，在每个空间区域上都预测一个mask proposal。这个方法存在以下三个缺点：

- **mask与特征的联系（局部一致性）丢失了**，如DeepMask中使用全连接网络去提取mask
- **特征的提取表示是冗余的**， 如DeepMask对每个前景特征都会去提取一次mask
- **下采样**（使用步长大于1的卷积）导致的位置信息丢失。

这个问题在mask rcnn上都没有啊

**自下而上的实例分割方法**

思路是：首先进行像素级别的语义分割，再通过聚类、度量学习等手段区分不同的实例。这种方法虽然保持了更好的低层特征（细节信息和位置信息），但也存在以下缺点：

- 对密集分割的质量要求很高，**会导致非最优的分割**
- **泛化能力较差**，无法应对类别多的复杂场景
- **后处理**方法繁琐

说白了就是非端到端的方式。

常规实例分割主流方法：

| Method     | AP   | AP50 | AP75 | APs  | APm  | APL  |
| :--------- | :--- | :--- | :--- | :--- | :--- | :--- |
| FCIS       | 29.2 | 49.5 |      | 7.1  | 31.3 | 50.0 |
| Mask R-CNN | 37.1 | 60.0 | 39.4 | 16.9 | 39.9 | 53.5 |
| YOLACT-700 | 31.2 | 50.6 | 32.8 | 12.1 | 33.3 | 47.1 |
| PolarMask  | 32.9 | 55.4 | 33.8 | 15.5 | 35.1 | 46.3 |
| SOLO       | 40.4 | 62.7 | 43.3 | 17.6 | 43.3 | 58.9 |
| PointRend  | 40.9 |      |      |      |      |      |
| BlendMask  | 41.3 | 63.1 | 44.6 | 22.7 | 44.1 | 54.5 |

## 双阶段分割

### mask rcnn

[详细解读](https://www.cnblogs.com/wangyong/p/10614898.html)

典型的基于检测的分割，利用backbone提取特征，并建立FPN网络，在FPN的每层特征图上利用1*1卷积形成W×H×3的anchor，筛选出与真实标签IOU较大的anchor作为RPN网络的训练标签。由RPN预测ROI，并在特征图上ROI对应位置进行ROI Align。最后进行候选框的回归和FCN得到mask图。

mask rcnn严重依赖检测框的准确性，边界框不准确，分割就会不准确；同时，mask rcnn在做分割时，未改动的mask rcnn的mask预测分支的，每个ROI只有28*28的mask分辨率，针对精细边界的分割严重不准确。

## 单阶段实例分割

### Instance-sensitive FCN

论文：InstanceFCN：Instance-sensitive Fully Convolutional Networks （2016）

这是一篇在FCN基础上，加入了instance-sensitive score maps实现对像素的实例分配。

虽然通过常规思维来说，FCN是对每个像素给一个类别标签，那么通过给每个像素一个更加细节的标签，比如C*K(C表示种类，K表示同一个种类的不同实例)，虽然这么做，在想法上是非常合理的，存在一个致命的缺陷：Conv是具有平移不变性的（[**How is a convolutional neural network able to learn invariant features?**](https://www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features)），通过卷积，不能分辨同一个类别的不同实例。



![](https://pic4.zhimg.com/80/v2-fb60b0652527186b789a220fc3f130cc_720w.jpg)

FCN将输入图片生成3\*3张相同大小的feature map，每一张特征图都代表对某个相对位置的响应，比如对于一个笔直站立的人，将人分为3*3的网格，那么头肯定是属于2号网格，应该在9张feature map中的响应中，第二张的响应最高。

![](https://img-blog.csdn.net/20180103212707845?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvamlvbmduaW1h/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

如上图所示，经过backbone（论文中选取的是VGG16）产生了3 \* 3的9张feature map（这9张特征图存在于9个通道），每张feature map的每个像素都产生一个前景/背景的置信度，如果是头部的像素，在第二张特征图的上，作为前景的响应就足够高，在其余图的响应就低。经过m*m（文中m=21）的滑窗，每个滑窗都分成九宫格，选取每个格子选取置信度最高的部分组成一个新的score map，这个操作就是assembling。（训练时的标签就是将每个实例作为9宫格划分，选取每个实例的对应部分为高值）。

![](https://pic3.zhimg.com/80/v2-8cc55f352749ef5cba5d0ca25467710c_720w.jpg)

这里论文没有说得很清楚，怎么组合all instances的，但是根据观察应该是对每一个滑窗产生的结果进行按原对应空间拼接。下面的分支也进行滑窗，生成是否有实例的objectness scores，最后结合highly scored instances图。

到这里都没有对实例进行分类，原论文中在后面接入了一个分类网络，对得到的实例图进行分类。

### PointRend




